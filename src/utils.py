import datetime
import hashlib
import os
import time
from concurrent.futures import ThreadPoolExecutor
from functools import wraps
from typing import Any, Callable, List, Literal, ParamSpec, TypeVar

import torch
import torch.nn as nn
import torch.nn.functional as F
from loguru import logger

T = TypeVar("T")
P = ParamSpec("P")


class MetricsUtils:
    @staticmethod
    def execution_time(func: Callable[P, T]):
        @wraps(func)
        def execution_time_wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
            start_time = time.perf_counter()
            result = func(*args, **kwargs)
            end_time = time.perf_counter()
            total_time = end_time - start_time
            logger.info(f"Function: {func.__name__} Took {total_time:.4f} seconds")
            return result

        return execution_time_wrapper


class TensorUtils:
    __DEBUG_MODE = False

    @staticmethod
    def log_details(torch_tensor: Any, tensor_name: str) -> None:
        if not TensorUtils.__DEBUG_MODE:
            return

        if isinstance(torch_tensor, torch.Tensor):
            print(tensor_name.ljust(24), type(torch_tensor).__name__, torch_tensor.dtype, torch_tensor.shape, sep="\t")
        else:
            print(f"{tensor_name} is not a tensor")


class JensenShannonUtils:
    def compute(
        self,
        P: torch.Tensor,
        Q: torch.Tensor,
        return_value: Literal["distance", "divergence"] = "distance",
        is_probability: bool = False,
        epsilon: float = 1e-8,
    ):
        """
        Computes the Jensen-Shannon Divergence or Distance between two sets of logits or probability distributions.

        Args:
            P (torch.Tensor):
                A tensor typically of shape (batch_size, num_classes), containing logits or probability values.
            Q (torch.Tensor):
                A tensor typically of shape (batch_size, num_classes), containing logits or probability values.
            return_value (Literal["distance", "divergence"], optional):
                Specifies whether to return the Jensen-Shannon "distance" (square root of divergence) or "divergence".
                Defaults to "distance".
            is_probability (bool, optional):
                If True, the inputs are treated as probabilities. If False, the inputs are considered logits and
                are converted to probabilities using softmax. Defaults to False.

        Returns:
            torch.Tensor:
                The computed Jensen-Shannon distance (if return_value is `distance`) or divergence
                (if return_value is `divergence`) between distributions P and Q.
        """
        p = P if is_probability else F.softmax(P, dim=1)
        q = Q if is_probability else F.softmax(Q, dim=1)

        m = 0.5 * (p + q + epsilon)

        log_p = torch.log(p + epsilon)
        log_q = torch.log(q + epsilon)

        kl_pm = F.kl_div(log_p, m, reduction="batchmean")
        kl_qm = F.kl_div(log_q, m, reduction="batchmean")

        divergence = 0.5 * (kl_pm + kl_qm)
        distance = torch.sqrt(divergence)

        return distance if return_value == "distance" else divergence


class ResultsUtils:
    def __init__(self):
        pass

    def parse_llm_outputs(
        self,
        prompts_without_special_tokens: List[str],
        llm_outputs_without_special_tokens: List[str],
    ):
        """
        Generates a new list containing only the new output generated by the LLM.
        In general LLM outputs contain both the input prompt and the generated output.
        This function takes the input prompts and the complete output generated by
        the model and parses out the newly generated text by the model.
        """
        if len(prompts_without_special_tokens) != len(llm_outputs_without_special_tokens):
            raise AssertionError("Each prompt should have a corresponding LLM generated output")

        generated_abstracts_map = map(
            lambda prompt, output: output[len(prompt) :].strip(),
            prompts_without_special_tokens,
            llm_outputs_without_special_tokens,
        )

        return list(generated_abstracts_map)


class TestResultsUtils(ResultsUtils):
    def __init__(self, max_thread_pool_workers: int, result_dir: str):
        super().__init__()
        self.verify_result_dir(result_dir)
        self.result_dir = result_dir
        self.thread_pool_executor = ThreadPoolExecutor(max_workers=max_thread_pool_workers)

    def verify_result_dir(self, result_dir: str):
        result_dir_path = result_dir if os.path.isabs(result_dir) else os.path.join(os.getcwd(), result_dir)
        if not os.path.exists(result_dir_path):
            logger.info(f"Test results directory {result_dir_path} does not exist. Creating directory...")
            os.makedirs(result_dir_path, exist_ok=True)
            logger.success(f"Test results directory created at {result_dir_path}.")

        if not os.path.isdir(result_dir_path):
            raise NotADirectoryError(f"Path {result_dir_path} is not a directory. Please provide a directory path.")

    def save_result(self, article: str, abstract: str) -> None:
        h = hashlib.md5(usedforsecurity=False)
        h.update(article.encode(encoding="utf-8"))
        article_hash = h.hexdigest()
        current_time = datetime.datetime.now()

        # `:` cannot be part of a valid file name in NTFS, as it is used as a delimeter for multiple files.
        # Refer https://stackoverflow.com/questions/54508733/python-3-not-raising-exception-for-invalid-file-name
        time_str = str(current_time).replace(":", "-").replace(" ", "_")

        article_filename = f"{time_str}_{article_hash}_article.txt"
        abstract_filename = f"{time_str}_{article_hash}_abstract.txt"
        try:
            with open(os.path.join(self.result_dir, article_filename), "w", encoding="utf-8") as article_file:
                article_file.write(article)
        except IOError as e:
            logger.error(f"ERRNO {e.errno}. {e.strerror}. Could not write ARTICLE file with hash {article_hash}")

        try:
            with open(os.path.join(self.result_dir, abstract_filename), "w", encoding="utf-8") as abstract_file:
                abstract_file.write(abstract)
        except IOError as e:
            logger.error(f"ERRNO {e.errno}. {e.strerror}. Could not write ABSTRACT file with hash {article_hash}")

    def save_result_list(self, articles: List[str], generated_abstracts: List[str]):
        for article, abstract in zip(articles, generated_abstracts):
            self.thread_pool_executor.submit(self.save_result, article, abstract)

    def parse_and_save(
        self,
        articles: List[str],
        prompts_without_special_tokens: List[str],
        llm_outputs_without_special_tokens: List[str],
    ):
        generated_abstracts = self.parse_llm_outputs(
            prompts_without_special_tokens,
            llm_outputs_without_special_tokens,
        )
        self.save_result_list(articles, generated_abstracts)

    def shutdown(self, wait: bool, cancel_futures: bool):
        self.thread_pool_executor.shutdown(wait=wait, cancel_futures=cancel_futures)
