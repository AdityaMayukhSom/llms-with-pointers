import os
from typing import Dict, Union

import torch
from loguru import logger
from transformers import LlamaForCausalLM, PreTrainedTokenizer, PreTrainedTokenizerFast
from transformers.generation import GenerateDecoderOnlyOutput, TextStreamer

from src.config import ScriptArguments
from src.constants import DataFilePaths, DataPointKeys
from src.dataset import batch_transform, get_dataset
from src.model import create_and_prepare_model
from src.utils import MetricsUtils, TestResultsUtils


@MetricsUtils.execution_time
def process_test_batch(
    *args,
    sample: Dict[DataPointKeys, str],
    device: torch.device,
    config: ScriptArguments,
    model: LlamaForCausalLM,
    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
    streamer: TextStreamer,
    test_result_utils: TestResultsUtils,
    **kwargs
):
    """
    Executes a single batch of test data.
    """
    prompts = sample.get(DataPointKeys.PROMPT)
    articles = sample.get(DataPointKeys.ARTICLE)
    abstracts = sample.get(DataPointKeys.ABSTRACT)

    if prompts is None:
        logger.error("Received `prompts` as None")
        return

    if articles is None:
        logger.error("Received `articles` as None")
        return

    if abstracts is None:
        logger.error("Received `abstracts` as None")
        return

    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
    ).to(device=device)

    outputs: GenerateDecoderOnlyOutput = model.generate(
        **inputs,
        max_new_tokens=config.max_tokens_to_generate_for_abstract,
        num_return_sequences=1,
        output_logits=True,
        output_scores=True,
        output_attentions=True,
        output_hidden_states=True,
        return_dict_in_generate=True,
        repetition_penalty=config.repetition_penalty,
        streamer=streamer,
    )

    # This is different from using the original prompts as this does not contain the
    # special tokens which are included in each prompt of the input, hence we need to
    # decode the tokens generated by original prompts with `skip_special_tokens` to
    # remove the tokens marked as special by the language model.
    prompts_without_special_tokens = tokenizer.batch_decode(
        inputs["input_ids"],
        skip_special_tokens=True,
    )

    # Note that each LLM output contain the prompt at the beginning. If `skip_special_tokens`
    # is set, then the initial prompt will too not have any special tokens and vice versa.
    llm_outputs_without_special_tokens = tokenizer.batch_decode(
        outputs.sequences,
        skip_special_tokens=True,
    )

    test_result_utils.parse_and_save(
        articles,
        prompts_without_special_tokens,
        llm_outputs_without_special_tokens,
        abstracts,
    )

    del llm_outputs_without_special_tokens
    del prompts_without_special_tokens
    del prompts
    del articles
    del abstracts
    del inputs
    del outputs


def model_test(config: ScriptArguments, device: torch.device):
    """
    Characterizing Mechanisms for Factual Recall in Language Models.
    `https://arxiv.org/pdf/2310.15910`

    TODO: Handle randomness in dataloader.
    https://pytorch.org/docs/stable/notes/randomness.html
    """

    if config.mode == "test" and config.data_dir is None:
        raise ValueError("Please provide a directory with the test data following the structure outlined in README.md.")

    if config.mode == "test" and config.test_result_dir is None:
        raise ValueError("Please specify a directory where the test results will be stored.")

    test_dataset = get_dataset(
        data_filename=DataFilePaths.TEST_DATA,
        index_filename=DataFilePaths.TEST_INDX,
        base_data_directory=config.data_dir,
    )

    test_loader = torch.utils.data.DataLoader(
        shuffle=False,
        dataset=test_dataset,
        batch_size=config.per_device_test_batch_size,
        collate_fn=lambda x: batch_transform(x, requested_max_words=config.requested_max_words),
    )

    model, tokenizer, _ = create_and_prepare_model(config, device=device)
    streamer = TextStreamer(tokenizer) if config.do_streaming_while_generating else None
    test_result_utils = TestResultsUtils(config.max_result_writers, config.test_result_dir)

    for sample in test_loader:
        process_test_batch(
            sample=sample,
            device=device,
            config=config,
            model=model,
            tokenizer=tokenizer,
            streamer=streamer,
            test_result_utils=test_result_utils,
        )

        # logger.info("output runtime type: {}".format(type(outputs).__name__))
        # logger.info("output.sequences runtime shape: {}".format(outputs.sequences.shape))
        # logger.info(outputs.sequences[:, inputs["input_ids"].shape[1] :].shape)
        # text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)
        # print(full_output_texts[0])
        # print("~" * 120)

    test_result_utils.shutdown(wait=True, cancel_futures=True)
