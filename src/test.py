# from concurrent.futures import ThreadPoolExecutor

import torch
from loguru import logger
from transformers.generation import GenerateDecoderOnlyOutput, TextStreamer

from src.config import ScriptArguments
from src.constants import DataPointKeys
from src.dataset import batch_transform, get_dataset
from src.model import create_and_prepare_model
from src.utils import parse_llm_outputs, save_test_results


def model_test(config: ScriptArguments, device: torch.device):
    """
    Characterizing Mechanisms for Factual Recall in Language Models.
    `https://arxiv.org/pdf/2310.15910`

    TODO: Handle randomness in dataloader.
    https://pytorch.org/docs/stable/notes/randomness.html
    """

    if config.mode == "test" and config.data_dir is None:
        raise ValueError("Please provide a directory with the test data following the structure outlined in README.md.")

    if config.mode == "test" and config.test_result_dir is None:
        raise ValueError("Please specify a directory where the test results will be stored.")

    model, tokenizer, _ = create_and_prepare_model(config, device=device)
    streamer = TextStreamer(tokenizer) if config.do_streaming_while_generating else None

    test_dataset = get_dataset(
        data_filename="single/tfrecord/test.tfrecord",
        index_filename="single/tfindex/test.tfindex",
        base_data_directory=config.data_dir,
    )

    test_loader = torch.utils.data.DataLoader(
        shuffle=False,
        dataset=test_dataset,
        batch_size=config.per_device_test_batch_size,
        collate_fn=lambda x: batch_transform(x, requested_max_words=config.requested_max_words),
    )

    # thread_pool_executor = ThreadPoolExecutor(max_workers=config.max_writer_processes)

    for sample in test_loader:
        prompts = sample.get(DataPointKeys.PROMPT)
        articles = sample.get(DataPointKeys.ARTICLE)
        abstracts = sample.get(DataPointKeys.ABSTRACT)

        if prompts is None:
            logger.error("Received `prompts` as None")
            continue

        if articles is None:
            logger.error("Received `articles` as None")
            continue

        if abstracts is None:
            logger.error("Received `abstracts` as None")
            continue

        inputs = tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
        ).to(device=device)

        outputs: GenerateDecoderOnlyOutput = model.generate(
            **inputs,
            max_new_tokens=config.max_tokens_to_generate_for_abstract,
            num_return_sequences=1,
            output_logits=True,
            output_scores=True,
            output_attentions=True,
            return_dict_in_generate=True,
            repetition_penalty=config.repetition_penalty,
            streamer=streamer,
        )

        # This is different from using the original prompts as this does not contain the
        # special tokens which are included in each prompt of the input, hence we need to
        # decode the tokens generated by original prompts with `skip_special_tokens` to
        # remove the tokens marked as special by the language model.
        prompts_without_special_tokens = tokenizer.batch_decode(
            inputs["input_ids"],
            skip_special_tokens=True,
        )

        # Note that each LLM output contain the prompt at the beginning. If `skip_special_tokens`
        # is set, then the initial prompt will too not have any special tokens and vice versa.
        llm_outputs_without_special_tokens = tokenizer.batch_decode(
            outputs.sequences,
            skip_special_tokens=True,
        )

        generated_abstracts = parse_llm_outputs(
            prompts_without_special_tokens,
            llm_outputs_without_special_tokens,
        )
        save_test_results(articles, generated_abstracts, config.test_result_dir)

        # thread_pool_executor.submit(
        #     save_test_results,
        #     full_input_texts,
        #     full_output_texts,
        #     config.test_result_dir,
        # )

        # logger.info("output runtime type: {}".format(type(outputs).__name__))
        # logger.info("output.sequences runtime shape: {}".format(outputs.sequences.shape))
        # logger.info(outputs.sequences[:, inputs["input_ids"].shape[1] :].shape)
        # text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)
        # print(full_output_texts[0])
        # print("~" * 120)

        del inputs
        del outputs

    # thread_pool_executor.shutdown(wait=True, cancel_futures=False)
